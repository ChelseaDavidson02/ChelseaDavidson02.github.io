# Data Sets

## Three types of data sets
In order to train a model in fastai, you need to create 3 separate sets of data - training, validation, and test. The first set of data is the **training** set which is the data which you fit your model to. The **validation** set is used to give you an understanding of how well your model behaves to unseen data. The **test** set is then used to test how well your model works. The validation set is intended to be an approximation of the test set and used to tune your model. If you directly used the test set to tune your model, it invalidates the results as you would modify your model until you know it works on this supposed "unseen" data set. 

Now that we know the purpose of each of the data sets, let's see how we make them.

## Building the different data sets
To build the different datasets, you can use Fastai's DataBlocks API. These DataBlocks allow you to easily define the training, validation, and test datasets. Let's explore how to create these datasets using DataBlocks:

### 1. Training Dataset

The training dataset is used to train the model. For the models I have used it with, it consists of a large set of labeled images. Here's an example of how to create a training dataset using DataBlocks:

```python
from fastai.vision.all import *

# Define the path to your training data
path = Path('path/to/training/data')

# Create a DataBlock for the training dataset
data_block = DataBlock(blocks=(ImageBlock, CategoryBlock),
                       get_items=get_image_files,
                       splitter=RandomSplitter(valid_pct=0.2, seed=42),
                       get_y=parent_label,
                       item_tfms=Resize(224)).dataloaders(path)
```

In the above example, we define the `blocks` argument as `(ImageBlock, CategoryBlock)` to indicate that our dataset consists of images and their corresponding labels. We use `get_image_files` to get the image file paths, `RandomSplitter` to split the data into training and validation sets, and `parent_label` to get the label from the parent folder name. We also apply the `Resize` transform to resize the images to a specific size (e.g., 224x224). It should be noted that for this to work, the images must be in subfolders which are named as the classification labels.

### 2. Validation Dataset

The validation dataset is used to evaluate the model's performance during training so that we can tune parameters without invalidating the results. When using the `RandomSplitter` in the DataBlock definition, as shown in the previous example, the validation dataset is automatically created based on the specified validation percentage. Here's how you can access this validation data:

```python
# Create the validation dataset
valid_dataset = data_block.valid
```

### 3. Test Dataset

The test dataset is used to assess the model's performance after training. It represents unseen data that the model has not encountered during training or validation. Here's an example of how to create a test dataset using DataBlocks:

```python
# Define the path to your test data
test_path = Path('path/to/test/data')

# Create the test dataset
test_dataset = data_block.test_dl(test_path)
```

By passing the test data path to `data_block.test_dl()`, you can create a test dataset that follows the same transformations as the training and validation datasets.


---
